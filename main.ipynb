{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install backtrader riskfolio-lib yfinance ta-lib kagglehub statsmodels\n",
    "%pip install pandas numpy matplotlib mplfinance gym stable-baselines3 tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation and Analysis Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for ta-lib (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [36 lines of output]\n",
      "      <string>:83: UserWarning: Cannot find ta-lib library, installation may fail.\n",
      "      C:\\Users\\kenne\\AppData\\Local\\Temp\\pip-build-env-zk9cnskn\\overlay\\Lib\\site-packages\\setuptools\\config\\_apply_pyprojecttoml.py:81: SetuptoolsWarning: `install_requires` overwritten in `pyproject.toml` (dependencies)\n",
      "        corresp(dist, value, root_dir)\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\\lib.win-amd64-cpython-312\\talib\n",
      "      copying talib\\abstract.py -> build\\lib.win-amd64-cpython-312\\talib\n",
      "      copying talib\\deprecated.py -> build\\lib.win-amd64-cpython-312\\talib\n",
      "      copying talib\\stream.py -> build\\lib.win-amd64-cpython-312\\talib\n",
      "      copying talib\\__init__.py -> build\\lib.win-amd64-cpython-312\\talib\n",
      "      running egg_info\n",
      "      writing ta_lib.egg-info\\PKG-INFO\n",
      "      writing dependency_links to ta_lib.egg-info\\dependency_links.txt\n",
      "      writing requirements to ta_lib.egg-info\\requires.txt\n",
      "      writing top-level names to ta_lib.egg-info\\top_level.txt\n",
      "      reading manifest file 'ta_lib.egg-info\\SOURCES.txt'\n",
      "      reading manifest template 'MANIFEST.in'\n",
      "      adding license file 'LICENSE'\n",
      "      adding license file 'AUTHORS'\n",
      "      writing manifest file 'ta_lib.egg-info\\SOURCES.txt'\n",
      "      copying talib\\_abstract.pxi -> build\\lib.win-amd64-cpython-312\\talib\n",
      "      copying talib\\_common.pxi -> build\\lib.win-amd64-cpython-312\\talib\n",
      "      copying talib\\_func.pxi -> build\\lib.win-amd64-cpython-312\\talib\n",
      "      copying talib\\_stream.pxi -> build\\lib.win-amd64-cpython-312\\talib\n",
      "      copying talib\\_ta_lib.c -> build\\lib.win-amd64-cpython-312\\talib\n",
      "      copying talib\\_ta_lib.pyi -> build\\lib.win-amd64-cpython-312\\talib\n",
      "      copying talib\\_ta_lib.pyx -> build\\lib.win-amd64-cpython-312\\talib\n",
      "      copying talib\\py.typed -> build\\lib.win-amd64-cpython-312\\talib\n",
      "      running build_ext\n",
      "      building 'talib._ta_lib' extension\n",
      "      creating build\\temp.win-amd64-cpython-312\\Release\\talib\n",
      "      \"C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -Ic:\\ta-lib\\c\\include \"-Ic:\\Program Files\\TA-Lib\\include\" \"-Ic:\\Program Files (x86)\\TA-Lib\\include\" -IC:\\Users\\kenne\\AppData\\Local\\Temp\\pip-build-env-zk9cnskn\\normal\\Lib\\site-packages\\numpy\\_core\\include -Ic:\\Users\\kenne\\OneDrive\\Desktop\\Quant_Workspace\\HFT-with-Deep-Reinforcement-Learning\\.venv\\include -IC:\\Users\\kenne\\AppData\\Local\\Programs\\Python\\Python312\\include -IC:\\Users\\kenne\\AppData\\Local\\Programs\\Python\\Python312\\Include \"-IC:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\" \"-IC:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\ATLMFC\\include\" \"-IC:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Auxiliary\\VS\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.22621.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\cppwinrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\include\\um\" /Tctalib/_ta_lib.c /Fobuild\\temp.win-amd64-cpython-312\\Release\\talib\\_ta_lib.obj\n",
      "      _ta_lib.c\n",
      "      talib/_ta_lib.c(1223): fatal error C1083: Cannot open include file: 'ta_libc.h': No such file or directory\n",
      "      error: command 'C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.43.34808\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for ta-lib\n",
      "ERROR: Failed to build installable wheels for some pyproject.toml based projects (ta-lib)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ta-lib\n",
      "  Using cached ta_lib-0.6.3.tar.gz (376 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: still running...\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: setuptools in c:\\users\\kenne\\onedrive\\desktop\\quant_workspace\\hft-with-deep-reinforcement-learning\\.venv\\lib\\site-packages (from ta-lib) (75.8.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\kenne\\onedrive\\desktop\\quant_workspace\\hft-with-deep-reinforcement-learning\\.venv\\lib\\site-packages (from ta-lib) (2.0.2)\n",
      "Building wheels for collected packages: ta-lib\n",
      "  Building wheel for ta-lib (pyproject.toml): started\n",
      "  Building wheel for ta-lib (pyproject.toml): finished with status 'error'\n",
      "Failed to build ta-lib\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m  \u001b[38;5;66;03m# Data manipulation and analysis\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m  \u001b[38;5;66;03m# Numerical operations\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m  \u001b[38;5;66;03m# Plotting and visualization\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmplfinance\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmpl\u001b[39;00m  \u001b[38;5;66;03m# Financial data plotting\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myfinance\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myf\u001b[39;00m \u001b[38;5;66;03m# Yahoo Finance API\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "%pip install ta-lib\n",
    "\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "import numpy as np  # Numerical operations\n",
    "import matplotlib.pyplot as plt  # Plotting and visualization\n",
    "import mplfinance as mpl  # Financial data plotting\n",
    "import yfinance as yf # Yahoo Finance API\n",
    "import talib as ta  # Technical analysis library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforement Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install stable-baselines3 package\n",
    "%pip install stable-baselines3 gym\n",
    "from stable_baselines3.common.evaluation import evaluate_policy  # Evaluate model\n",
    "\n",
    "import gym\n",
    "from stable_baselines3 import PPO  # Proximal Policy Optimization algorithm\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv  # Vectorized environment\n",
    "from stable_baselines3.common.callbacks import EvalCallback  # Evaluation callback\n",
    "from stable_baselines3.common.env_checker import check_env  # Check environment\n",
    "\n",
    "# Install stable-baselines3 package\n",
    "%pip install stable-baselines3\n",
    "from stable_baselines3.common.evaluation import evaluate_policy  # Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  # Standardizing features by removing the mean and scaling to unit variance\n",
    "from sklearn.preprocessing import MinMaxScaler  # Scaling features to a range (default is 0 to 1) or custom range (min and max)\n",
    "from sklearn.preprocessing import RobustScaler  # Scaling features using statistics that are robust to outliers\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier  # Random forest classifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error  # Accuracy classification score\n",
    "from sklearn.metrics import classification_report, confusion_matrix  # Classification report and confusion matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score  # ROC curve and ROC AUC score\n",
    "\n",
    "from sklearn.model_selection import train_test_split  # Splitting data into training and testing sets\n",
    "from sklearn.model_selection import RandomizedSearchCV  # Randomized search on hyperparameters\n",
    "from sklearn.model_selection import cross_val_score  # Cross validation score\n",
    "from sklearn.model_selection import TimeSeriesSplit  # Time Series split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Llbraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf  # Deep learning framework (alternatively, you can use PyTorch)\n",
    "import torch \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = yf.download(\"GOOG\", start=\"2010-01-01\", end=\"2021-01-01\")\n",
    "\n",
    "\n",
    "for i in [5,10,15,20,50,100]:\n",
    "    data['sma'+str(i)] = data['Close'].rolling(window=i).mean()\n",
    "    data['ema'+str(i)] = data['Close'].ewm(span=i, adjust=False).mean()\n",
    "    data['momentum'+str(i)] = data['Close']-data['Close'].shift(i)\n",
    "    data['rsi'+str(i)] = ta.momentum.rsi(data['Close'], n=i, fillna=True)\n",
    "    data['macd'+str(i)] = ta.trend.macd(data['Close'], n_fast=i, n_slow=2*i, fillna=True)\n",
    "    data['atr'+str(i)] = ta.volatility.average_true_range(data['High'], data['Low'], data['Close'], n=i, fillna=True)\n",
    "    data['bollinger_up'+str(i)] = ta.volatility.bollinger_hband(data['Close'], n=i, ndev=2, fillna=True)\n",
    "    data['bollinger_down'+str(i)] = ta.volatility.bollinger_lband(data['Close'], n=i, ndev=2, fillna=True)\n",
    "   \n",
    "\n",
    "data['returns'] = data['Close'].pct_change()\n",
    "data['direction'] = np.where(data['returns'] > 0, 1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data['Close'], label='Close Price')\n",
    "plt.plot(data['sma5'], label='SMA 5')\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Stock Price with SMA 5\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for accuracy score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.dropna().drop(['Close'], axis=1)\n",
    "target = np.where(data['Close'].shift(-1) > data['Close'], 1, -1)  # Target variable\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Standardizing the features\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Training the random forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Calculating the mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print('Mean Squared Error:', mse)\n",
    "\n",
    "# Calculating the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networking Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class torch_bot():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tensor_bot():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences for training\n",
    "def create_sequences(data: pd.DataFrame, seq_length: int)-> np.array:\n",
    "  sequences = []\n",
    "  for i in range(len(data) - seq_length):\n",
    "       sequences.append(data[i:i+seq_length])\n",
    "\n",
    "  return np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "data = yf.download(\"AAPL\", start = \"2010-01-01\", end = \"2019-12-31\")\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "scaled_data = scaler.fit_transform(data['Close'].values.reshape(-1,1))\n",
    "\n",
    "# Prepare the training and test datasets\n",
    "train_size = int(len(scaled_data) * 0.8)\n",
    "train_data = scaled_data[:train_size]\n",
    "test_data = scaled_data[train_size:]\n",
    "\n",
    "# Build the LSTM Model\n",
    "seq_length = 60\n",
    "X_train = create_sequences(train_data, seq_length)\n",
    "X_test = create_sequences(test_data, seq_length)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units = 50, return_sequences = True, input_shape = (seq_length, 1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units = 50, return_sequences = False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units = 1))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, train_data[seq_length:], batch_size = 32, epochs = 5)\n",
    "\n",
    "# Predict stock prices\n",
    "X_test = create_sequences(test_data, seq_length)\n",
    "predictions = model.predict(X_test)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "print(pd.DataFrame(predictions).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcment Learning Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"fin_data.csv\", index_col = \"Datetime\")\n",
    "df.index = pd.to_datetime(df.index)\n",
    "\n",
    "df = df.drop(columns = [\"Volume\", \"Open\", \"High\", \"Low\", \"Adj Close\"])\n",
    "\n",
    "# Target and test split variable\n",
    "train_size = 10000\n",
    "train_df = df.iloc[:train_size]\n",
    "test_df = df.iloc[train_size:]\n",
    "\n",
    "# Q-Learning discrete coninuous features\n",
    "num_bins = 11\n",
    "states = num_bins**5 * 3  # total number of states\n",
    "\n",
    "# state - Action Pairs\n",
    "states * 3\n",
    "\n",
    "train_df.returns.hist(bins = 100)\n",
    "plt.show()\n",
    "\n",
    "train_df['returns_binned'], bin_edges = pd.qcut(train_df.returns, \n",
    "                                                q = num_bins, labels = False,\n",
    "                                                retbins = True, duplicates = 'drop')\n",
    "\n",
    "train_df.returns_binned.value_counts().sort_index()\n",
    "print(bin_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning the testset\n",
    "test_df['returns_binned'] = pd.cut(test_df.returns, \n",
    "                                   bins = np.concatenate(([-np.inf], bin_edges[1:-1], [np.inf])), \n",
    "                                   labels = False)\n",
    "test_df.returns_binned.value_counts().sort_index()\n",
    "test_df.returns.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges_list = []\n",
    "num_bins = 11\n",
    "features = [\"SMA_ratio\", \"MACD_hist\", \"RSI\", \"SO_diff\", \"returns\"]\n",
    "\n",
    "for feature in features:\n",
    "    train_df[feature + \"_binned\"], bin_edges = pd.qcut(train_df[feature], \n",
    "                                                        q = num_bins, labels = False, \n",
    "                                                        retbins = True, duplicates = 'drop')\n",
    "    bin_edges_list.append(bin_edges)\n",
    "\n",
    "print(feature, bin_edges)\n",
    "\n",
    "train_df.RSI_binned.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trading Profits and Rewards\n",
    "units = 1000\n",
    "train_df[\"Profit\"] = train_df.Close.diff() * units\n",
    "train_df.dropna(inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RL_Bot():\n",
    "    def calculate_reward(position, Profit):\n",
    "        \"\"\" Calculate reward based on the position and profit \"\"\"\n",
    "        if position == 2:  # Long position\n",
    "            return Profit  # Reward as is positive\n",
    "        elif position == 0: # Short position\n",
    "            return -Profit  # Reward is negative \n",
    "        else: # No position\n",
    "            return 0 # No position, no reward\n",
    "\n",
    "    def get_state(row):\n",
    "        \"\"\" Get the state based on the row \"\"\"\n",
    "        return (int(row.returns_binned), \n",
    "                int(row.SMA_ratio_binned),\n",
    "                int(row.MACD_hist_binned), \n",
    "                int(row.RSI_binned), \n",
    "                int(row.SO_diff_binned), \n",
    "                int(row.position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0,1 # Learning rate\n",
    "gamma = 0.95 # Discount factor\n",
    "epsilon = 0.5 # Exploration factor\n",
    "num_bins = (11, 11, 11, 11, 11, 3, 3) # Action space\n",
    "\n",
    "q_table = np.random.uniform(low = -1, high = 1, size = num_bins)\n",
    "\n",
    "total_reward = 0\n",
    "start_index = 0\n",
    "\n",
    "# select the sequence of all timestamps\n",
    "data = train_df.iloc[start_index:].copy()\n",
    "data[\"position\"] = 1 # initial position\n",
    "\n",
    "raw = data.iloc[0] # first step in episode\n",
    "\n",
    "state = RL_Bot.get_state(raw) # state: starting with neutral position and current indcitators\n",
    "\n",
    "for step in range(len(train_df) - 1):\n",
    "    # Epsilon-greedy action slection\n",
    "    if np.random.rand() < epsilon:\n",
    "        action = np.random.choice(3) # Random action\n",
    "    else:\n",
    "        action = np.argmax(q_table[state]) # Greedy action\n",
    "        \n",
    "    # Update the position based on the action\n",
    "    next_position = action\n",
    "    \n",
    "    # Get the next row\n",
    "    next_row = data.iloc[step + 1]\n",
    "    \n",
    "    # Calculate the reward based on the current position and the rows reward\n",
    "    reward = RL_Bot.calculate_reward(data.position, next_row.Profit)\n",
    "    \n",
    "    # Determine the next state\n",
    "    next_state = RL_Bot.get_state(next_row)\n",
    "    \n",
    "    # Update the Q-table\n",
    "    best_next_action = np.argmax(q_table[next_state])\n",
    "    td_target = reward + gamma * q_table[next_state][best_next_action]\n",
    "    q_table[state][action] += alpha * (td_target - q_table[state][action])\n",
    "    \n",
    "    # update the total reward and state\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "\n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.position = data.position.map({\n",
    "    0: -1,\n",
    "    1: 0,\n",
    "    2:1\n",
    "})\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"train_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training an RL Agent with Q-Tables (multiple Episodes)\n",
    "train_df = pd.read_csv(\"train_df.csv\")\n",
    "\n",
    "total_timestaps = len(train_df)\n",
    "print(\"Timestamps:\", total_timestaps)\n",
    "\n",
    "maxsteps = 250 # number of bars in one subset\n",
    "\n",
    "start_index = np.radnom.randint(0, total_timestaps - maxsteps + 1)\n",
    "print(\"Start Index:\", start_index)\n",
    "\n",
    "data = train_df.iloc[start_index:start_index + maxsteps].copy()\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for Q-learning\n",
    "num_episodes = 1000\n",
    "seed = 100\n",
    "np.random.seed(seed)\n",
    "max_steps = 250\n",
    "alpha = 0.1 # Learning rate\n",
    "gamma = 0.95 # Discount factor\n",
    "epsilon = 1.0 # Exploration factor\n",
    "epsilon_decay = 0.999 # Exploration decay\n",
    "\n",
    "num_bins = (11, 11, 11, 11, 11, 3, 3) # Action space\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    start_index = np.random.randint(0, total_timestaps - maxsteps + 1)\n",
    "    data = train_df.iloc[start_index:start_index + maxsteps].copy()\n",
    "    data[\"position\"] = 1 # initial position\n",
    "    row = data.iloc[0] # first step in episode\n",
    "    state = RL_Bot.get_state(row) # state: starting with neutral position and current indicators\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps - 1):\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.choice(3)\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])\n",
    "        \n",
    "        # Detemine the next position\n",
    "        next_position = action\n",
    "        \n",
    "        # update the position for the next row\n",
    "        data.at[data.index[step + 1], 'position'] = next_position\n",
    "        \n",
    "        # Get the next row\n",
    "        next_row = data.iloc[step + 1]\n",
    "        \n",
    "        # Calculate the reward based on the current position and the rows reward\n",
    "        reward = RL_Bot.calculate_reward(data.position, data.Profit)\n",
    "        \n",
    "        # Determine the next state\n",
    "        next_state = RL_Bot.get_state(next_row)\n",
    "        \n",
    "        # Update the Q-table\n",
    "        best_next_action = np.argmax(q_table[next_state])\n",
    "        td_target = reward + gamma * q_table[next_state][best_next_action]\n",
    "        q_table[state][action] += alpha * (td_target - q_table[state][action])\n",
    "        \n",
    "        # Update the total reward and state\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "    # Record performance metrices\n",
    "    total_rewards.append(total_reward)\n",
    "    \n",
    "    # Check if the episode is done\n",
    "    if total_reward > 0:\n",
    "        successful_episodes = True\n",
    "    else:\n",
    "        successful_episodes = False\n",
    "        \n",
    "    success_count += episode_success\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "            \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
